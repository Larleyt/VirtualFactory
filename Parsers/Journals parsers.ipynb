{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "import os\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(root_dir=None, prefix=None):\n",
    "    file_paths = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(root_dir):\n",
    "        for name in files:\n",
    "            if name.decode(\"utf-8\").lower().startswith(prefix):\n",
    "                file_paths.append(os.path.join(root, name))\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_journal(fpath, output_path):\n",
    "    xls = pd.ExcelFile(fpath)\n",
    "\n",
    "    # Parse Status sheet\n",
    "    df = xls.parse(\n",
    "        sheetname=0, \n",
    "        header=0,\n",
    "        index_col=0,\n",
    "        parse_cols=\"B:E\"\n",
    "    )\n",
    "    \n",
    "    df.to_csv(\n",
    "        os.path.join(output_path, 'СТАТУС.csv'), \n",
    "        encoding=\"utf-8\", \n",
    "        sep=\"\\t\"\n",
    "    )\n",
    "    \n",
    "    # Parse Machinetools sheets \n",
    "    for sheet_idx in range(1, 13):\n",
    "        df = xls.parse(\n",
    "            sheetname=sheet_idx, \n",
    "            header=1,\n",
    "            parse_cols=\"B:N\",\n",
    "            skiprows=[2]\n",
    "        )\n",
    "        \n",
    "        # Cпособ найти индекс строки, где начинается ненужная часть таблицы\n",
    "        # -4 нужно для багованных листов, где 00:00 проставлено до конца листа :(\n",
    "        try:\n",
    "            max_idx = df.loc[df.isnull().all(axis=1)].index[0]\n",
    "        except IndexError:\n",
    "            max_idx = -4\n",
    "        \n",
    "        filename = xls.sheet_names[sheet_idx].encode(\"utf-8\") + \"_станок.csv\"\n",
    "        df[:max_idx].to_csv(\n",
    "            os.path.join(output_path, filename), \n",
    "            encoding=\"utf-8\",\n",
    "            sep=\"\\t\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "def parse_plan(fpath, output_path):\n",
    "    xls = pd.ExcelFile(fpath)\n",
    "    xls.sheet_names\n",
    "\n",
    "    df = xls.parse(\n",
    "        sheetname=0,\n",
    "        header=0,\n",
    "        index_col=None\n",
    "    ).dropna(how='all', axis=1).dropna(how='all')\n",
    "    # find out num of header rows\n",
    "    headers_num = 0\n",
    "    for index, row in df.iterrows():\n",
    "        if not np.isnan(index):\n",
    "            break\n",
    "        headers_num += 1\n",
    "\n",
    "    # ~ old\n",
    "    # max_multiindex_row = df.loc[\n",
    "    #     df[u\"Наименование\"].str.contains(u\"Заказ\", case=False, na=False)\n",
    "    # ]\n",
    "    \n",
    "    df = df[df[u\"Наименование\"].notnull()].reset_index(drop=False)\n",
    "    # Create order_num column\n",
    "    # df.insert(0, column=u\"whatever\", value=np.nan)\n",
    "    df = df.reindex(method='ffill')\n",
    "    \n",
    "    print len(df.columns)\n",
    "    \n",
    "    xx = xls.parse(\n",
    "        sheetname=0,\n",
    "        header=None\n",
    "    )[0:headers_num].dropna(how='all', axis=1).dropna(how='all')\n",
    "    # xx.insert(0, column=u\"whatever\", value=u\"Заказ\")\n",
    "    xx.iloc[0,0] = u\"Номер детали\"\n",
    "    # Fill down\n",
    "    xx.fillna(method='ffill', axis=0, inplace=True)\n",
    "    # Fill across\n",
    "    # xx = xx.fillna(method='ffill', axis=1)\n",
    "    xx.to_csv(\n",
    "        os.path.join(output_path, 'multi_index.csv'),\n",
    "        header=False,\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    mxx = pd.read_csv(\n",
    "        os.path.join(output_path, 'multi_index.csv'), \n",
    "        header=[0,1,2],\n",
    "        skipinitialspace=True,\n",
    "        tupleize_cols=True,\n",
    "        encoding=\"utf-8\",\n",
    "    )\n",
    "    print len(df.columns)\n",
    "    print len(mxx.columns)\n",
    "    df.columns = pd.MultiIndex.from_tuples(mxx.columns)\n",
    "    \n",
    "    df.to_csv(\n",
    "        os.path.join(output_path, \"parsed_План.csv\"), \n",
    "        encoding=\"utf-8\",\n",
    "        sep=\"\\t\"\n",
    "    )\n",
    "    return df.head()\n",
    "    \n",
    "    # dff = xls.parse(\n",
    "    #     sheetname=0,\n",
    "    #     header=[0,1,2],\n",
    "    #     index_col=None\n",
    "    # )\n",
    "    # dff = dff.reindex(method='ffill')\n",
    "    # dff.fillna(method='ffill', axis=0, inplace=True)\n",
    "    # dff = dff.fillna(method='ffill', axis=1)\n",
    "    # ~\n",
    "\n",
    "    # # Headers as rows for now\n",
    "    # df = xls.parse(sheetname=0, header=None, index_col=None)\n",
    "    # \n",
    "    # # Create order_num column\n",
    "    # #df.insert(0, column=u\"whatever\", value=np.nan)\n",
    "    # \n",
    "    # headers = df.iloc[:headers_num]\n",
    "    # \n",
    "    # # Fill down\n",
    "    # headers = headers.fillna(method='ffill')\n",
    "    # # fill few column indicies (fucked up indexing)\n",
    "    # headers.iloc[0,0] = u\"Номер заказа\"\n",
    "    # headers.iloc[0,1] = u\"Номер детали\"\n",
    "    # \n",
    "    # # Fill across (carefully)\n",
    "    # headers_not_to_fill = headers.iloc[:,:5]\n",
    "    # \n",
    "    # headers_to_fill = headers.iloc[:,5:]\n",
    "    # headers = pd.concat([\n",
    "    #     headers_not_to_fill,\n",
    "    #     headers_to_fill.fillna(method='ffill', axis=1)\n",
    "    # ], axis=1)\n",
    "    # \n",
    "    # df = df.iloc[headers_num:]\n",
    "    # df = df.reset_index(drop=True)\n",
    "\n",
    "    # Create multiindex column names\n",
    "    # df.columns = pd.MultiIndex.from_arrays(mxx.values.tolist())\n",
    "    # dup_first_index_dates = df.columns\n",
    "    # print df.head()\n",
    "    # blah = df[u\"Наименование\"] #.ix[df[u\"Наименование\"].notnull()].index.tolist()\n",
    "    # print blah\n",
    "    # max_multiindex_row = df.loc[\n",
    "    #     df[u\"Наименование\"].str.contains(u\"Заказ\", case=False, na=False)\n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_journals(fpaths, output_path):\n",
    "    for xls_path in fpaths:\n",
    "        print \"Processing status sheets: \" + str(xls_path)\n",
    "        xls = pd.ExcelFile(xls_path)\n",
    "    \n",
    "        # Parse Status sheet\n",
    "        df_status = xls.parse(\n",
    "            sheetname=0, \n",
    "            header=0,\n",
    "            index_col=0,\n",
    "            parse_cols=\"B:E\"\n",
    "        )\n",
    "        \n",
    "        df_status.to_csv(\n",
    "            os.path.join(output_path, 'СТАТУС.csv'), \n",
    "            encoding=\"utf-8\", \n",
    "            sep=\"\\t\"\n",
    "        )\n",
    "    \n",
    "    # Parse Machinetools sheets\n",
    "    machinetool_names = []\n",
    "    machinetool_frames = []\n",
    "    for sheet_idx in range(1, 13):\n",
    "        print \"Processing machinetools sheet for each journal: \" + str(sheet_idx)\n",
    "        year_frames = []\n",
    "        \n",
    "        for xls_path in fpaths:\n",
    "            xls = pd.ExcelFile(xls_path)\n",
    "\n",
    "            df = xls.parse(\n",
    "                sheetname=sheet_idx,\n",
    "                header=1,\n",
    "                parse_cols=\"B:N\",\n",
    "                skiprows=[2]\n",
    "            ).dropna(how='all', axis=1).dropna(how='all')\n",
    "            \n",
    "            if any(df[u\"Время старт (чч:мм)\"] == u\"Статус\"):\n",
    "                df = df[:-4]\n",
    "            year_frames.append(df)\n",
    "            \n",
    "        machinetool_names.append(xls.sheet_names[sheet_idx])\n",
    "        machinetool_df = pd.concat(year_frames, ignore_index=True)\n",
    "        machinetool_frames.append(machinetool_df)\n",
    "        # filename = xls.sheet_names[sheet_idx].encode(\"utf-8\") + \"_станок.csv\"\n",
    "    \n",
    "    result = pd.concat(\n",
    "        machinetool_frames, \n",
    "        keys=machinetool_names,\n",
    "        names=[u\"Станок\", u\"Index\"]\n",
    "    )\n",
    "    result.to_csv(\n",
    "        os.path.join(output_path, u\"Журнал станков за 3 месяца.csv\"),\n",
    "        encoding=\"utf-8\",\n",
    "        sep=\"\\t\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing status sheets: /home/larleyt/fl/ML/VirtualFactory/data collection/21_01_2017/Отредактированное/НОВЫЙ Журнал загрузки станков Январь для заполнения.xls\nProcessing status sheets: /home/larleyt/fl/ML/VirtualFactory/data collection/21_01_2017/Отредактированное/НОВЫЙ Журнал загрузки станков Декабрь для заполнения.xls\nProcessing status sheets: /home/larleyt/fl/ML/VirtualFactory/data collection/21_01_2017/Отредактированное/НОВЫЙ Журнал загрузки станков Ноябрь для заполнения.xls\nProcessing machinetools sheet for each journal: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing machinetools sheet for each journal: 2\nProcessing machinetools sheet for each journal: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing machinetools sheet for each journal: 4\nProcessing machinetools sheet for each journal: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing machinetools sheet for each journal: 6\nProcessing machinetools sheet for each journal: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing machinetools sheet for each journal: 8\nProcessing machinetools sheet for each journal: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing machinetools sheet for each journal: 10\nProcessing machinetools sheet for each journal: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing machinetools sheet for each journal: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing plans /home/larleyt/fl/ML/VirtualFactory/data collection/21_01_2017/План 2016 (2).xls\n"
     ]
    }
   ],
   "source": [
    "ROOT_DIR = r\"/home/larleyt/fl/ML/VirtualFactory/data collection\"\n",
    "OUTPUT_JOURNALS_PATH = ROOT_DIR + r\"/CSVs/Journals\"\n",
    "\n",
    "journal_file_paths = find_files(ROOT_DIR, u\"новый\")\n",
    "plan_file_paths = find_files(ROOT_DIR, u\"план 2016\")\n",
    "\n",
    "parse_journals(journal_file_paths, OUTPUT_JOURNALS_PATH)\n",
    "\n",
    "# for xls_path in journal_file_paths:\n",
    "#     print \"Processing journals: \" + xls_path\n",
    "#     xls_named_dir = os.path.join(\n",
    "#         OUTPUT_JOURNALS_PATH, \n",
    "#         os.path.basename(xls_path).split(\".\")[0])\n",
    "#     print xls_path\n",
    "#     if not os.path.exists(xls_named_dir):\n",
    "#         os.mkdir(xls_named_dir)\n",
    "#     parse_journal(xls_path, xls_named_dir)\n",
    "\n",
    "\n",
    "for xls_path in plan_file_paths:\n",
    "    print \"Processing plans \" + xls_path\n",
    "# \n",
    "#     # temporary:\n",
    "#     xls_named_dir = os.path.join(\n",
    "#         OUTPUT_PATH, \n",
    "#         os.path.basename(xls_path).split(\".\")[0])\n",
    "#     if not os.path.exists(xls_named_dir):\n",
    "#         os.mkdir(xls_named_dir)\n",
    "#     parse_plan(xls_path, xls_named_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}